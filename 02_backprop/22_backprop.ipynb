{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "98cd42b9fa51e2cba8c897a349e339ef",
     "grade": false,
     "grade_id": "cell-87195ccb7e06731c",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Number of points for this notebook:</b> 2\n",
    "<br>\n",
    "<b>Deadline:</b> March 9, 2020 (Monday). 23:00\n",
    "</div>\n",
    "\n",
    "# Exercise 2.2. Backpropagation. Batch processing.\n",
    "\n",
    "In this task, we implement MLP blocks that allow batch processing: processing multiple training examples at the same time. And we also perform training of an MLP on a toy data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9eed5501920d3e124c40a66dcee99fa7",
     "grade": false,
     "grade_id": "cell-cafdead5e95c3773",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "skip_training = True  # Set this flag to True before validation and submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "fe00604c3ac013b22df1df7f9c7f175c",
     "grade": true,
     "grade_id": "evaluation_settings",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# During grading, this cell sets skip_training to True\n",
    "# skip_training = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7431268c0145f4d724f4c57bc3d3501b",
     "grade": false,
     "grade_id": "cell-14c19df006e22022",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "def numerical_gradient(fun, x, eps=1e-4):\n",
    "    \"\"\"Compute derivatives of a given function fun numerically.\n",
    "    \n",
    "    Args:\n",
    "      fun: A python function fun(x) which accepts a vector argument (one-dimensional numpy array)\n",
    "           and returns a vector output (one-dimensional numpy array).\n",
    "      x:   An input vector for which the numerical gradient should be computed.\n",
    "      eps: A scalar which defines the magnitude of perturbations applied to the inputs.\n",
    "\n",
    "    Returns:\n",
    "      gnum: A two-dimensional array in which an element in row i and column j is the partial derivative of the\n",
    "            i-th output of function fun wrt j-th input of function fun (computed numerically).\n",
    "    \"\"\"\n",
    "    assert x.ndim <= 1, \"Only vector inputs are supported\"\n",
    "    e = np.zeros_like(x)\n",
    "    f = fun(x)\n",
    "    assert f.ndim <= 1, \"Only vector outputs are supported\"\n",
    "    gnum = np.zeros((f.size, x.size))\n",
    "    for i in range(len(x)):\n",
    "        e[:] = 0\n",
    "        e[i] = 1\n",
    "        f1, f2 = fun(x + e*eps), fun(x - e * eps)\n",
    "        gnum[:, i] = (f1 - f2) / (2 * eps)\n",
    "    return gnum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "53df92f44dca82230c9f03414cc7a037",
     "grade": false,
     "grade_id": "cell-51cc37ff548d6c9d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "In this notebook, we will need two blocks from the first part of the assignment. Please copy your implementations of `MSELoss` and `Tanh` to the cells below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "03ca6ceb430ac63faa4e05b7ea99a298",
     "grade": false,
     "grade_id": "MSELoss",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class MSELoss:\n",
    "    def forward(self, y, target):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "          y (array):      Inputs of the loss function (can be, e.g., an output of a neural network),\n",
    "                           shape (xsize,).\n",
    "          target (array): Targets, shape (xsize,).\n",
    "        \"\"\"\n",
    "        self.diff = diff = y - target  # Keep this for backward computations\n",
    "        c = np.sum(np.square(diff)) / diff.size\n",
    "        return c\n",
    "\n",
    "    def backward(self):\n",
    "        \"\"\"\n",
    "        Returns:\n",
    "          dy (array): Gradient of the MSE loss wrt the inputs, shape (xsize,).\n",
    "        \"\"\"\n",
    "        assert hasattr(self, 'diff'), \"Need to call forward() first\"\n",
    "        return 2*(self.diff)/len(self.diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8aac44f04219fb804fb36ec4e3f2356e",
     "grade": false,
     "grade_id": "Tanh",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class Tanh:\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "          x (array): Input of shape (xsize,).\n",
    "        \n",
    "        Returns:\n",
    "          y (array): Output of shape (xsize,).\n",
    "        \"\"\"\n",
    "        self.x = x\n",
    "        \n",
    "        return np.tanh(x)\n",
    "\n",
    "    def backward(self, dy):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "          dy (array): Gradient of a loss wrt outputs, shape (xsize,).\n",
    "        \n",
    "        Returns:\n",
    "          dx (array): Gradient of a loss wrt inputs, shape (xsize,).\n",
    "        \"\"\"\n",
    "        assert hasattr(self, 'x'), \"Need to call forward() first.\"\n",
    "        return (1- np.tanh((self.x))**2)*dy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d7431697c584f338e7904299702a3187",
     "grade": false,
     "grade_id": "cell-b71f50dda717743d",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## 1. Linear layer which supports batch processing\n",
    "\n",
    "In the cell below, we implement forward and backward computations for a linear layer which supports batch processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c66bd5eca3b95d39609c507c9e6433f7",
     "grade": false,
     "grade_id": "cell-225997cdacc80572",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def linear_forward_batch(x, W, b):\n",
    "    \"\"\"Forward computations in the linear layer:\n",
    "        y = W x + b\n",
    "\n",
    "    Args:\n",
    "      x (array): Inputs of shape (batch_size, xsize).\n",
    "      W (array): Weight matrix of shape (ysize, xsize).\n",
    "      b (array): Bias term of shape (ysize,).\n",
    "\n",
    "    Returns:\n",
    "      y (array): Outputs of shape (batch_size, ysize).\n",
    "    \"\"\"\n",
    "    return np.matmul(x, W.T) + b\n",
    "\n",
    "def linear_backward_batch(dy, x, W, b):\n",
    "    \"\"\"Backward computations in the linear layer.\n",
    "\n",
    "    Args:\n",
    "      dy (array): Gradient of a loss wrt outputs, shape (batch_size, ysize).\n",
    "      x (array): Input of shape (batch_size, xsize).\n",
    "      W (array): Weight matrix of shape (ysize, xsize).\n",
    "      b (array): Bias term of shape (ysize,).\n",
    "\n",
    "    Returns:\n",
    "      dx (array): Gradient of a loss wrt inputs, shape (batch_size, xsize).\n",
    "      dW (array): Gradient wrt weight matrix W, shape (ysize, xsize).\n",
    "      db (array): Gradient wrt bias term b, shape (ysize,).\n",
    "    \"\"\"\n",
    "    assert dy.ndim == 2 and dy.shape[1] == W.shape[0]\n",
    "    dx = np.matmul(dy,W)\n",
    "    dW = np.matmul(dy.T, x)\n",
    "    db = np.sum(dy, axis=0)\n",
    "    return dx, dW, db\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "86ecbf548a7176053ee9186d7d651353",
     "grade": false,
     "grade_id": "cell-7d17ae70e7b767ae",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analytical gradient:\n",
      " [[-15.30144927   1.69245584]\n",
      " [-16.12850193  -0.91774969]\n",
      " [-16.95555459  -3.52795523]]\n",
      "Numerical gradient:\n",
      " [[-15.30144927   1.69245584]\n",
      " [-16.12850193  -0.91774969]\n",
      " [-16.95555459  -3.52795523]]\n",
      "Success\n"
     ]
    }
   ],
   "source": [
    "def test_linear_batch():\n",
    "    batch_size = 4\n",
    "    x = np.random.randn(batch_size, 2)\n",
    "    W = np.random.randn(3, 2)\n",
    "    b = np.random.randn(3)\n",
    "\n",
    "    # Test shapes\n",
    "    y = linear_forward_batch(x, W, b)\n",
    "    dy = np.arange(batch_size * 3).reshape((batch_size, 3))\n",
    "    dx, dW, db = linear_backward_batch(dy, x, W, b)\n",
    "    assert dx.shape == x.shape, f\"Bad dx.shape={dx.shape}, x.shape={x.shape}\"\n",
    "    assert dW.shape == W.shape, f\"Bad dW.shape={dW.shape}, W.shape={W.shape}\"\n",
    "    assert db.shape == b.shape, f\"Bad db.shape={db.shape}, b.shape={b.shape}\"\n",
    "\n",
    "    # Test gradient wrt W numerically\n",
    "    print('Analytical gradient:\\n', dW)\n",
    "    dW_num = numerical_gradient(lambda W: linear_forward_batch(x, W.reshape(3, 2), b).flatten(), W.flatten())\n",
    "    dW_num = dW_num.reshape(y.shape + W.shape)\n",
    "    expected = (dy[:, :, None, None] * dW_num).sum(axis=(0,1))\n",
    "    print('Numerical gradient:\\n', expected)\n",
    "    assert np.allclose(dW, expected), 'Analytical and numerical results differ'\n",
    "    print('Success')\n",
    "\n",
    "test_linear_batch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4137e6ce36250cf8abafae53b40c87c7",
     "grade": false,
     "grade_id": "cell-91d19db375ecd0d5",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "We recommend you to compare analytical and numerical computations of the gradients also wrt input `x` and bias term `b`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "90b048fd2e8f3db3d29b5c49c50ac5c3",
     "grade": true,
     "grade_id": "linear_batch_Wb",
     "locked": true,
     "points": 0.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# This cell tests linear_batch_forward and linear_batch_backward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b5523b22dc3de49b096304280fe2f255",
     "grade": true,
     "grade_id": "cell-6bad54818463363a",
     "locked": true,
     "points": 0.25,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# This cell tests linear_batch_forward and linear_batch_backward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a05cc72b42f869144f79dd1576051754",
     "grade": true,
     "grade_id": "linear_batch_x",
     "locked": true,
     "points": 0.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# This cell tests linear_batch_forward and linear_batch_backward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3ad522c578be014878b1fca82fcb6947",
     "grade": false,
     "grade_id": "cell-96395e08fccd7200",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Now we define a linear layer which supports batch processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6ae3da9815f962a602dc0aac95e2765b",
     "grade": false,
     "grade_id": "cell-bc17d247e1fbdf32",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class LinearBatch:\n",
    "    def __init__(self, in_features, out_features):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "          in_features (int): Number of input features which should be equal to xsize.\n",
    "          out_features (out): Number of output features which should be equal to ysize.\n",
    "        \"\"\"\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        \n",
    "        # Initialization of the weights\n",
    "        bound = 3 / np.sqrt(in_features)\n",
    "        self.W = np.random.uniform(-bound, bound, (out_features, in_features))\n",
    "        bound = 1 / np.sqrt(in_features)\n",
    "        self.b = np.random.uniform(-bound, bound, out_features)\n",
    "\n",
    "        self.grad_W = None\n",
    "        self.grad_b = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "          x (array): Inputs of shape (batch_size, xsize).\n",
    "        \n",
    "        Returns:\n",
    "          y (array): Outputs of shape (batch_size, ysize).\n",
    "        \"\"\"\n",
    "        self.x = x  # Keep this for backward computations\n",
    "        return linear_forward_batch(x, self.W, self.b)\n",
    "\n",
    "    def backward(self, dy):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "          dy (array): gradient of a loss wrt outputs, shape (batch_size, ysize).\n",
    "        \n",
    "        Returns:\n",
    "          dx (array): gradient of a loss wrt inputs, shape (batch_size, xsize).\n",
    "        \"\"\"\n",
    "        assert hasattr(self, 'x'), \"Need to call forward() first\"\n",
    "        assert dy.ndim == 2 and dy.shape[1] == self.W.shape[0]\n",
    "        dx, self.grad_W, self.grad_b = linear_backward_batch(dy, self.x, self.W, self.b)\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4777c87115128f50ad0e0164768030a7",
     "grade": false,
     "grade_id": "cell-9275de846357269d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# We can now create a linear layer which supports batch processing, ...\n",
    "layer = LinearBatch(in_features=3, out_features=2)\n",
    "\n",
    "# do forward computations ...\n",
    "batch_size = 4\n",
    "x = np.random.randn(batch_size, 3)\n",
    "y = layer.forward(x)\n",
    "\n",
    "# and backward computations\n",
    "dy = np.arange(batch_size * 2).reshape((batch_size, 2))\n",
    "dx = layer.backward(dy)\n",
    "\n",
    "# We now have the gradients computed\n",
    "# wrt input x\n",
    "assert dx.shape == x.shape, f\"Bad dx.shape: {dx.shape}, x.shape={x.shape}\"\n",
    "# wrt weight matrix W\n",
    "assert layer.grad_W.shape == layer.W.shape, f\"Bad grad_W.shape: {layer.grad_W.shape}, W.shape={layer.W.shape}\"\n",
    "# wrt bias term b\n",
    "assert layer.grad_b.shape == layer.b.shape, f\"Bad grad_b.shape={layer.grad_b.shape}, b.shape={layer.b.shape}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2e1fef7f019d850ffc1cee5bff68da8c",
     "grade": false,
     "grade_id": "cell-15e28241c83862b3",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## 2. MLP implementation which supports batch processing\n",
    "\n",
    "In the cell below, implement an MLP with two hidden layers and `Tanh` nonlinearity which supports batch processing. Use instances of classes `LinearBatch` and `Tanh` in your implementation. These instances should be attributes of class `MLPBatch` such as attribute `fc1` in the example below:\n",
    "```\n",
    "    def __init__(self, in_features, hidden_size1, hidden_size2, out_features):\n",
    "        self.fc1 = LinearBatch(...)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5245446b4dafa9a8489c7def877e9e1a",
     "grade": false,
     "grade_id": "MLPBatch",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "class MLPBatch:\n",
    "    def __init__(self, in_features, hidden_size1, hidden_size2, out_features):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "          in_features (int): Number of inputs which should be equal to xsize.\n",
    "          hidden_size1 (int): Number of units in the first hidden layer.\n",
    "          hidden_size2 (int): Number of units in the second hidden layer.\n",
    "          out_features (int): Number of outputs which should be equal to ysize.\n",
    "        \"\"\"\n",
    "        self.fc1 = LinearBatch(in_features, hidden_size1)\n",
    "        self.activation_fn1 = Tanh()\n",
    "        self.fc2 = LinearBatch(hidden_size1, hidden_size2)\n",
    "        self.activation_fn2 = Tanh()\n",
    "        self.fc3 = LinearBatch(hidden_size2, out_features)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "          x (array): Input of shape [batch_size, xsize].\n",
    "        \n",
    "        Returns:\n",
    "          y (array): Output of shape [batch_size, ysize].\n",
    "         \"\"\"\n",
    "        x = self.activation_fn1.forward(self.fc1.forward(x))\n",
    "        x = self.activation_fn2.forward(self.fc2.forward(x))\n",
    "        x = self.fc3.forward(x)\n",
    "        return x\n",
    "\n",
    "    def backward(self, dy):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "          dy (array): Gradient of a loss wrt outputs (shape [batch_size, ysize]).\n",
    "        \n",
    "        Returns:\n",
    "          dx (array): Gradient of a loss wrt inputs (shape [batch_size, xsize]).\n",
    "        \"\"\"\n",
    "        dx = self.activation_fn2.backward(self.fc3.backward(dy))\n",
    "        dx = self.activation_fn1.backward(self.fc2.backward(dx))\n",
    "        dx = self.fc1.backward(dx)\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a4517087a3fac10667886a688b23fcf1",
     "grade": false,
     "grade_id": "cell-07c80ef21983d673",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success\n"
     ]
    }
   ],
   "source": [
    "def test_MLPBatch_shapes():\n",
    "    batch_size = 10\n",
    "    x = np.random.randn(batch_size, 1)\n",
    "    mlp_batch = MLPBatch(1, 10, 20, 1)\n",
    "    y = mlp_batch.forward(x)\n",
    "\n",
    "    dy = np.arange(batch_size).reshape((batch_size, 1))   # Dummy gradient of a loss function wrt MLP's outputs.\n",
    "    dx = mlp_batch.backward(dy)\n",
    "    assert dx.shape == x.shape, f\"Bad dx.shape={dx.shape}, x.shape={x.shape}\"\n",
    "    print('Success')\n",
    "\n",
    "test_MLPBatch_shapes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ace99f8ef95c6ba8aec27f263a4bcac0",
     "grade": true,
     "grade_id": "test_MLPBatch",
     "locked": true,
     "points": 0.75,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# This cell tests MLPBatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a5b95c8c59605d0902557809dd6750e5",
     "grade": false,
     "grade_id": "cell-91d50afcfcbc324d",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'MLPBatch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-7a65c02ac45c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinspace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mmlp_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMLPBatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmlp_batch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'MLPBatch' is not defined"
     ]
    }
   ],
   "source": [
    "# Let's create an MLP with random weights and compute the gradients wrt the inputs\n",
    "batch_size = 100\n",
    "x = np.linspace(-10, 10, batch_size)\n",
    "mlp_batch = MLPBatch(1, 10, 20, 1)\n",
    "y = mlp_batch.forward(x.reshape((batch_size, 1))).flatten()\n",
    "\n",
    "dy_dx = mlp_batch.backward(np.ones((batch_size, 1))).flatten()\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(x, y)\n",
    "ax.plot(x, dy_dx)\n",
    "ax.grid(True)\n",
    "ax.legend(['y', 'dy_dx'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "fbb865aabe7b04a79776e7f6fed6750f",
     "grade": false,
     "grade_id": "cell-d5136f1291f0c36a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "You can visually inspect whether the computations of the derivative seem correct.\n",
    "\n",
    "More importantly, we can compute the gradient of a loss wrt the parameters of the MLP. The gradients can be used to update the parameters using gradient descent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6f6fcdd4102626693e5ba198e5fbc96a",
     "grade": false,
     "grade_id": "cell-0630dc5ad992327d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## 3. Training MLP network with backpropagation\n",
    "\n",
    "Now let us use our code to train an MLP network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2e0b1798b19d46a30fd9585db6867ec8",
     "grade": false,
     "grade_id": "cell-bb746d106b37391b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f69b8ee8ba8>]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Let us generate toy data\n",
    "np.random.seed(2)\n",
    "x = np.random.randn(100, 1)\n",
    "x = np.sort(x, axis=0)\n",
    "targets = np.sin(x * 2 * np.pi / 3)\n",
    "targets = targets + 0.2 * np.random.randn(*targets.shape)\n",
    "\n",
    "# Plot the data\n",
    "fig, ax = plt.subplots(1)\n",
    "ax.plot(x, targets, '.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f58005ded24101541643cec6ff0a9137",
     "grade": false,
     "grade_id": "cell-b5925a7912f3191c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# And train an MLP network using gradient descent\n",
    "from IPython import display\n",
    "\n",
    "if not skip_training:  # The trained MLP is not tested\n",
    "    mlp = MLPBatch(1, 10, 10, 1)  # Create MLP network\n",
    "    loss = MSELoss()  # Create loss\n",
    "\n",
    "    fig, ax = plt.subplots(1)\n",
    "    ax.plot(x, targets, '.')\n",
    "    learning_rate = 0.15\n",
    "    n_epochs = 1 if skip_training else 200\n",
    "    for i in range(n_epochs):\n",
    "        # Forward computations\n",
    "        y = mlp.forward(x)\n",
    "        c = loss.forward(y, targets)\n",
    "\n",
    "        # Backward computations\n",
    "        dy = loss.backward()\n",
    "        dx = mlp.backward(dy)\n",
    "\n",
    "        # Gradient descent update\n",
    "        learning_rate *= 0.99  # Learning rate annealing\n",
    "        for module in mlp.__dict__.values():\n",
    "            if hasattr(module, 'W'):\n",
    "                module.W = module.W - module.grad_W * learning_rate\n",
    "                module.b = module.b - module.grad_b * learning_rate\n",
    "\n",
    "        ax.clear()\n",
    "        ax.plot(x, targets, '.')\n",
    "        ax.plot(x, y, 'r-')\n",
    "        ax.grid(True)\n",
    "        ax.set_title('Iteration %d/%d' % (i+1, n_epochs))\n",
    "        display.clear_output(wait=True)\n",
    "        display.display(fig)\n",
    "        plt.pause(0.005)\n",
    "    display.clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "778ff4525fa210606bebf85f115fb080",
     "grade": false,
     "grade_id": "cell-ca3ef83db74fc3f9",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "If you implement the MLP correctly, you will see that the learned function fits the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "295b11424526bb9dc1ea17bf141803b7",
     "grade": false,
     "grade_id": "cell-841919d678edba70",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Conclusions</b>\n",
    "</div>\n",
    "\n",
    "Now you have implemented backpropagation and trained an MLP network using gradient descent.\n",
    "\n",
    "PyTorch makes it easier to create neural networks with different architectures and optimize its parameters using (variants of) gradient descent:\n",
    "* It contains multiple building blocks with forward and backward computations implemented.\n",
    "* It implements optimization methods that work well for neural networks.\n",
    "* Computations can be performed either on GPU or CPU using the same code."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc-autonumbering": false,
  "toc-showtags": false,
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
